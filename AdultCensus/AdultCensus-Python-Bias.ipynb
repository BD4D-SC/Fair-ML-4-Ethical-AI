{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "<a href=\"http://www.insa-toulouse.fr/\" ><img src=\"http://www.math.univ-toulouse.fr/~besse/Wikistat/Images/logo-insa.jpg\" style=\"float:left; max-width: 120px; display: inline\" alt=\"INSA\"/></a> \n",
    "\n",
    "<a href=\"http://www.univ-tlse3.fr/\" ><img src=\"http://www.univ-tlse3.fr/medias/photo/ut3pres_logoq_1372757033342.jpg?ID_FICHE=49702\" style=\"float:right; max-width: 250px; display: inline\" alt=\"INSA\"/></a> \n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# Biais et Discrimination en Apprentissage Statistique\n",
    "## Détection et correction du biais  sur les données `Adult Income` avec <a href=\"https://www.python.org/\"><img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/f/f8/Python_logo_and_wordmark.svg/390px-Python_logo_and_wordmark.svg.png\" style=\"max-width: 120px; display: inline\" alt=\"Python\"/></a>\n",
    "\n",
    "### Résumé\n",
    "Utilisation des fonctions du [dépôt](https://github.com/algofairness/fairness-comparison) de  [Friedler et al. 2019](https://dl.acm.org/citation.cfm?id=3287589) pour tester différents outils de correction des biais d'apprentissage.\n",
    "\n",
    "## Introduction\n",
    "### Outils\n",
    "Friedler et al. (2019) proposent une comparaison systématique de plusieurs algorithmes de réparation des données ou d'apprentissage loyal sur 5 jeux de données classiques: *Adult Income, Ricci, German Bank, Propublica recidivism, Propublica violent recidivism*. Dans la discussion finale, les auteurs insistent lourdement sur les choix opérés lors du prétraitement des données, la prolifération des mesures statistiques de biais et l'instabilité de la phase d'apprentissage. \n",
    "- Le prétraitement doit être rigoureusement identique avant l'application de différents algorithmes pour pouvoir en comparer les performances.\n",
    "- Il suffit de se limiter à quelques mesures de biais car celles-ci sont fortement corrélées . Prendre par exemple en compte l'effet disproportionné (*disparate impact* ou *DI*) et une comparaison des taux d'erreur conditionnels.\n",
    "- Il est important de reproduire les résultats sur plusieurs séparations aléatoires apprentissage - test des données.\n",
    "\n",
    "    Friedler et al. (2019) analysent donc les résultats de 10 exécutions opérées sur 5 jeux de données et pour une liste de 19 algorithmes combinant différents apprentissages (naïf bayésien, régression logistique, SVM, arbre de décision) et correction (Calders, Zafar, Kamishima, Feldman) ou pas du biais. Par souci de reproductibilité de la recherche, le code python est disponible dans un [dépôt](https://github.com/algofairness/fairness-comparison) public. Il peut être installé avec la commande: \n",
    "    \n",
    "    `pip3 install fairness`\n",
    "\n",
    "### Objectif\n",
    "*Attention* ce dépôt n'est pas une librairie comme peut l'être `scikit-learn` mais le code des programmes exécutant tous les prétraitements, les comparaisons de performances et la production des graphiques. En conséquence, l'exécution totale est excessivement longue et ne présente que peu d'intérêt. En revanche et dans l'attente de la réalisation d'une librairie efficace, il peut être utile ou intéressant de pouvoir mettre en oeuvre ces ressources comme pour produire les diagnostics et résultats d'un algorithme sur un jeu de données spécifique. C'est ce qui est tenté ci-dessous.\n",
    "\n",
    "### Problèmes rencontrés\n",
    "- Prétraitements: les auteurs ont à coeur de produire des codes génériques suffisamment généraux pour être exécutables sans intervention manuelle experte sur les données. C'est sûrement très positif pour une plus grande automatisation et standadisation des codes mais ne corrige pas des incohérences, erreurs de codification... pouvant être présentes et en fait toujours présentes dans des données réelles. D'autre part ils ne procèdent pas à des regroupements de modalités, par exemple en lien avec l'origine des individus. Certaines variables, dont celle sensible d'origine, possèdent un nombre important de modalités avec certaines de faible effectif donc sans réel intérêt. Cela complique inutilement les traitements et la production de résultats.\n",
    "- Ce sont des codes itératifs exécutant sytématiquement toutes les combinaisons des paramètres. Exécution très longue, il serait préférable de pouvoir faire des choix (données algorithme, paramètre) mais les fonctions ne sont pas documentées et donc difficilement adaptables à des besoins spécifiques.\n",
    "- Problème de mise à jour et compatibilité de certaines librairies entraînant la production de nombreux *warning* et des erreurs d'exécution pour certains algorithmes; tous les résultats ne peuvent pas être reproduits.\n",
    "- Volume des résultats: les objectifs de l'article conduivent à calculer tous les critères possibles pour finalement montrer que ceux-ci sont très liés et que quelques uns suffisent à caractériser les biais et la qualité de prévision; extraire les plus pertinents des fichiers de sortie n'est pas si simple.\n",
    "\n",
    "#### Questions\n",
    "- Il reste une incohérence à expliquer entre les résultats de ce calepin avec les résultats produits par le calepin en R sur les mêmes données. En moyenne, l'accroissement du biais de la précison est plus important lorsque les algorithmes sont exécutés en R (20 itérations) plutôt qu'en Python (10 itérations). Cela concerne les arbres de décision et la régression logistique. La seule différence évidente entre les deux codes concerne le pré-traitement des données; manuel en R automatique en Python. \n",
    "- A faire: récupérer les données transformées issues du dépôt python pour les traiter en R.\n",
    "- Les traitements longs et complexes de réparation des données ou d'apprentissage sous contrainte de loyauté sont-ils justifiés? Ne suffit-il pas de corriger naïvement, biaiser en faveur du groupe défavorisé, le seuil de la prise de décision de l'algorithme d'apprentissage retenu? Cf. Calders et Verwer (2010) pour le classifieur bayésien naïf. \n",
    "\n",
    "#### Conclusion\n",
    "Une fois ces tâches de comparaison réalisées et les bons outils sélectionnés, il serait pertinent de produire une librairie efficace car parallélisée et industrialisable de correction du biais pour aborder des problèmes de la vraie vie et pas seulement des jeux de données publics mais élémentaires.\n",
    "\n",
    "\n",
    "Les codes exécutés ci-dessous sont extraits du [dépôt](https://github.com/algofairness/fairness-comparison) associé à  l'article de Friedler et al. (2019).\n",
    "## Prétraitements\n",
    "Les codes ci-dessous permettent de transformer les données initiales des 5 fichiers pour les rendre compatibles avec les traitements suivants. il s'agit avant tout de recoder les variables qualitatives, éventuellement celles quantitatives et de supprimer toutes les observations avec des données manquantes.\n",
    "#### Librairies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "import fire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fairness.data.objects.list import DATASETS, get_dataset_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "fairness.data.objects.Adult.Adult"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Choix d'un seul jeu de données\n",
    "Le process n'est appliqué qu'au jeude données `adult income` à titre d'illustration. De toute façon, les données tranformées sont enregistrées sans le dépôt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<fairness.data.objects.Adult.Adult at 0x7fd09d2d7908>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DATASETS[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(dataset_names = get_dataset_names()):\n",
    "\n",
    "    for dataset in DATASETS:\n",
    "        if not dataset.get_dataset_name() in dataset_names:\n",
    "            continue\n",
    "        print(\"--- Processing dataset: %s ---\" % dataset.get_dataset_name())\n",
    "        data_frame = dataset.load_raw_dataset()\n",
    "        d = preprocess(dataset, data_frame)\n",
    "        \n",
    "        for k, v in d.items():\n",
    "            write_to_file(dataset.get_filename(k), v)\n",
    "\n",
    "def write_to_file(filename, dataframe):\n",
    "    print(\"Writing data to: %s\" % filename)\n",
    "    dataframe.to_csv(filename, index = False)\n",
    "\n",
    "def preprocess(dataset, data_frame):\n",
    "    \"\"\"\n",
    "    The preprocess function takes a pandas data frame and returns two modified data frames:\n",
    "    1) all the data as given with any features that should not be used for training or fairness\n",
    "    analysis removed.\n",
    "    2) only the numerical and ordered categorical data, sensitive attributes, and class attribute.\n",
    "    Categorical attributes are one-hot encoded.\n",
    "    3) the numerical data (#2) but with a binary (numerical) sensitive attribute\n",
    "    \"\"\"\n",
    "\n",
    "    # Remove any columns not included in the list of features to keep.\n",
    "    smaller_data = data_frame[dataset.get_features_to_keep()]\n",
    "\n",
    "    # Handle missing data.\n",
    "    missing_processed = dataset.handle_missing_data(smaller_data)\n",
    "\n",
    "    # Remove any rows that have missing data.\n",
    "    missing_data_removed = missing_processed.dropna()\n",
    "    missing_data_count = missing_processed.shape[0] - missing_data_removed.shape[0]\n",
    "    if missing_data_count > 0:\n",
    "        print(\"Missing Data: \" + str(missing_data_count) + \" rows removed from dataset \" +  \\\n",
    "              dataset.get_dataset_name())\n",
    "\n",
    "    # Do any data specific processing.\n",
    "    processed_data = dataset.data_specific_processing(missing_data_removed)\n",
    "\n",
    "    print(\"\\n-------------------\")\n",
    "    print(\"Balance statistics:\")\n",
    "    print(\"\\nClass:\")\n",
    "    print(dataset.get_class_balance_statistics(processed_data))\n",
    "    print(\"\\nSensitive Attribute:\")\n",
    "    for r in dataset.get_sensitive_attribute_balance_statistics(processed_data):\n",
    "        print(r)\n",
    "        print(\"\\n\")\n",
    "    print(\"\\n\")\n",
    "\n",
    "    # Handle multiple sensitive attributes by creating a new attribute that's the joint distribution\n",
    "    # of all of those attributes.  For example, if a dataset has both 'Race' and 'Gender', the\n",
    "    # combined feature 'Race-Gender' is created that has attributes, e.g., 'White-Woman'.\n",
    "    sensitive_attrs = dataset.get_sensitive_attributes()\n",
    "    if len(sensitive_attrs) > 1:\n",
    "        new_attr_name = '-'.join(sensitive_attrs)\n",
    "        ## TODO: the below may fail for non-string attributes\n",
    "        processed_data = processed_data.assign(temp_name =\n",
    "                             processed_data[sensitive_attrs].apply('-'.join, axis=1))\n",
    "        processed_data = processed_data.rename(columns = {'temp_name' : new_attr_name})\n",
    "        # dataset.append_sensitive_attribute(new_attr_name)\n",
    "        # privileged_joint_vals = '-'.join(dataset.get_privileged_class_names(\"\"))\n",
    "        # dataset.get_privileged_class_names(\"\").append(privileged_joint_vals)\n",
    "\n",
    "    # Create a one-hot encoding of the categorical variables.\n",
    "    processed_numerical = pd.get_dummies(processed_data,\n",
    "                                         columns = dataset.get_categorical_features())\n",
    "\n",
    "    # Create a version of the numerical data for which the sensitive attribute is binary.\n",
    "    sensitive_attrs = dataset.get_sensitive_attributes_with_joint()\n",
    "    privileged_vals = dataset.get_privileged_class_names_with_joint(\"\")\n",
    "    processed_binsensitive = make_sensitive_attrs_binary(\n",
    "        processed_numerical, sensitive_attrs, privileged_vals)\n",
    "\n",
    "    # Create a version of the categorical data for which the sensitive attributes is binary.\n",
    "    processed_categorical_binsensitive = make_sensitive_attrs_binary(\n",
    "        processed_data, sensitive_attrs,\n",
    "        dataset.get_privileged_class_names(\"\")) ## FIXME\n",
    "    # Make the class attribute numerical if it wasn't already (just for the bin_sensitive version).\n",
    "    class_attr = dataset.get_class_attribute()\n",
    "    pos_val = dataset.get_positive_class_val(\"\") ## FIXME\n",
    "\n",
    "    processed_binsensitive = make_class_attr_num(processed_binsensitive, class_attr, pos_val)\n",
    "\n",
    "    return { \"original\": processed_data,\n",
    "             \"numerical\": processed_numerical,\n",
    "             \"numerical-binsensitive\": processed_binsensitive,\n",
    "             \"categorical-binsensitive\": processed_categorical_binsensitive }\n",
    "\n",
    "def make_sensitive_attrs_binary(dataframe, sensitive_attrs, privileged_vals):\n",
    "    newframe = dataframe.copy()\n",
    "    for attr, privileged in zip(sensitive_attrs, privileged_vals):\n",
    "        # replace privileged vals with 1\n",
    "        newframe[attr] = newframe[attr].replace({ privileged : 1 })\n",
    "        # replace all other vals with 0\n",
    "        newframe[attr] = newframe[attr].replace(\"[^1]\", 0, regex = True)\n",
    "    return newframe\n",
    "\n",
    "def make_class_attr_num(dataframe, class_attr, positive_val):\n",
    "    # don't change the class attribute unless its a string (pandas type: object)\n",
    "    if (dataframe[class_attr].dtypes == 'object'):\n",
    "        dataframe[class_attr] = dataframe[class_attr].replace({ positive_val : 1 })\n",
    "        dataframe[class_attr] = dataframe[class_attr].replace(\"[^1]\", 0, regex = True)\n",
    "    return dataframe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing Data: 2399 rows removed from dataset adult\n",
      "\n",
      "-------------------\n",
      "Balance statistics:\n",
      "\n",
      "Class:\n",
      "income-per-year\n",
      "<=50K    22654\n",
      ">50K      7508\n",
      "dtype: int64\n",
      "\n",
      "Sensitive Attribute:\n",
      "race\n",
      "Amer-Indian-Eskimo      286\n",
      "Asian-Pac-Islander      895\n",
      "Black                  2817\n",
      "Other                   231\n",
      "White                 25933\n",
      "dtype: int64\n",
      "\n",
      "\n",
      "sex\n",
      "Female     9782\n",
      "Male      20380\n",
      "dtype: int64\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataset=DATASETS[1]\n",
    "data_frame = dataset.load_raw_dataset()\n",
    "d = preprocess(dataset, data_frame)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attention, les données transformées sont sauvegardées par défaut dans l'architecture locale du dépôt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing data to: /home-local/pbesse/anaconda3/lib/python3.6/site-packages/fairness/data/preprocessed/adult_original.csv\n",
      "Writing data to: /home-local/pbesse/anaconda3/lib/python3.6/site-packages/fairness/data/preprocessed/adult_numerical.csv\n",
      "Writing data to: /home-local/pbesse/anaconda3/lib/python3.6/site-packages/fairness/data/preprocessed/adult_numerical-binsensitive.csv\n",
      "Writing data to: /home-local/pbesse/anaconda3/lib/python3.6/site-packages/fairness/data/preprocessed/adult_categorical-binsensitive.csv\n"
     ]
    }
   ],
   "source": [
    "for k, v in d.items():\n",
    "            write_to_file(dataset.get_filename(k), v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exécution\n",
    "#### Librairie et fonctions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available algorithms:\n",
      "  SVM\n",
      "  GaussianNB\n",
      "  LR\n",
      "  DecisionTree\n",
      "  Kamishima\n",
      "  Calders\n",
      "  ZafarBaseline\n",
      "  ZafarFairness\n",
      "  ZafarAccuracy\n",
      "  Kamishima-accuracy\n",
      "  Kamishima-DIavgall\n",
      "  Feldman-SVM\n",
      "  Feldman-GaussianNB\n",
      "  Feldman-LR\n",
      "  Feldman-DecisionTree\n",
      "  Feldman-SVM-DIavgall\n",
      "  Feldman-SVM-accuracy\n",
      "  Feldman-GaussianNB-DIavgall\n",
      "  Feldman-GaussianNB-accuracy\n"
     ]
    }
   ],
   "source": [
    "import fire\n",
    "import os\n",
    "import statistics\n",
    "import sys\n",
    "\n",
    "from fairness import results\n",
    "from fairness.data.objects.list import DATASETS, get_dataset_names\n",
    "from fairness.data.objects.ProcessedData import ProcessedData\n",
    "from fairness.algorithms.list import ALGORITHMS\n",
    "from fairness.metrics.list import get_metrics\n",
    "\n",
    "from fairness.algorithms.ParamGridSearch import ParamGridSearch\n",
    "\n",
    "NUM_TRIALS_DEFAULT = 10\n",
    "\n",
    "def get_algorithm_names():\n",
    "    result = [algorithm.get_name() for algorithm in ALGORITHMS]\n",
    "    print(\"Available algorithms:\")\n",
    "    for a in result:\n",
    "        print(\"  %s\" % a)\n",
    "    return result\n",
    "\n",
    "def run(num_trials = NUM_TRIALS_DEFAULT, dataset = get_dataset_names(),\n",
    "        algorithm = get_algorithm_names()):\n",
    "    algorithms_to_run = algorithm\n",
    "\n",
    "    print(\"Datasets: '%s'\" % dataset)\n",
    "    for dataset_obj in DATASETS:\n",
    "        if not dataset_obj.get_dataset_name() in dataset:\n",
    "            continue\n",
    "\n",
    "        print(\"\\nEvaluating dataset:\" + dataset_obj.get_dataset_name())\n",
    "\n",
    "        processed_dataset = ProcessedData(dataset_obj)\n",
    "        train_test_splits = processed_dataset.create_train_test_splits(num_trials)\n",
    "\n",
    "        all_sensitive_attributes = dataset_obj.get_sensitive_attributes_with_joint()\n",
    "        for sensitive in all_sensitive_attributes:\n",
    "\n",
    "            print(\"Sensitive attribute:\" + sensitive)\n",
    "\n",
    "            detailed_files = dict((k, create_detailed_file(\n",
    "                                          dataset_obj.get_results_filename(sensitive, k),\n",
    "                                          dataset_obj,\n",
    "                                          processed_dataset.get_sensitive_values(k), k))\n",
    "                for k in train_test_splits.keys())\n",
    "\n",
    "            for algorithm in ALGORITHMS:\n",
    "                if not algorithm.get_name() in algorithms_to_run:\n",
    "                    continue\n",
    "\n",
    "                print(\"    Algorithm: %s\" % algorithm.get_name())\n",
    "                print(\"       supported types: %s\" % algorithm.get_supported_data_types())\n",
    "                if algorithm.__class__ is ParamGridSearch:\n",
    "                    param_files =  \\\n",
    "                        dict((k, create_detailed_file(\n",
    "                                     dataset_obj.get_param_results_filename(sensitive, k,\n",
    "                                                                            algorithm.get_name()),\n",
    "                                     dataset_obj, processed_dataset.get_sensitive_values(k), k))\n",
    "                          for k in train_test_splits.keys())\n",
    "                for i in range(0, num_trials):\n",
    "                    for supported_tag in algorithm.get_supported_data_types():\n",
    "                        train, test = train_test_splits[supported_tag][i]\n",
    "                        try:\n",
    "                            params, results, param_results =  \\\n",
    "                                run_eval_alg(algorithm, train, test, dataset_obj, processed_dataset,\n",
    "                                             all_sensitive_attributes, sensitive, supported_tag)\n",
    "                        except Exception as e:\n",
    "                            import traceback\n",
    "                            traceback.print_exc(file=sys.stderr)\n",
    "                            print(\"Failed: %s\" % e, file=sys.stderr)\n",
    "                        else:\n",
    "                            write_alg_results(detailed_files[supported_tag],\n",
    "                                              algorithm.get_name(), params, i, results)\n",
    "                            if algorithm.__class__ is ParamGridSearch:\n",
    "                                for params, results in param_results:\n",
    "                                    write_alg_results(param_files[supported_tag],\n",
    "                                                      algorithm.get_name(), params, i, results)\n",
    "\n",
    "            print(\"Results written to:\")\n",
    "            for supported_tag in algorithm.get_supported_data_types():\n",
    "                print(\"    %s\" % dataset_obj.get_results_filename(sensitive, supported_tag))\n",
    "\n",
    "            for detailed_file in detailed_files.values():\n",
    "                detailed_file.close()\n",
    "\n",
    "def write_alg_results(file_handle, alg_name, params, run_id, results_list):\n",
    "    line = alg_name + ','\n",
    "    params = \";\".join(\"%s=%s\" % (k, v) for (k, v) in params.items())\n",
    "    line += params + (',%s,' % run_id)\n",
    "    line += ','.join(str(x) for x in results_list) + '\\n'\n",
    "    file_handle.write(line)\n",
    "\n",
    "def run_eval_alg(algorithm, train, test, dataset, processed_data, all_sensitive_attributes,\n",
    "                 single_sensitive, tag):\n",
    "    \"\"\"\n",
    "    Runs the algorithm and gets the resulting metric evaluations.\n",
    "    \"\"\"\n",
    "    privileged_vals = dataset.get_privileged_class_names_with_joint(tag)\n",
    "    positive_val = dataset.get_positive_class_val(tag)\n",
    "\n",
    "    # get the actual classifications and sensitive attributes\n",
    "    actual = test[dataset.get_class_attribute()].values.tolist()\n",
    "    sensitive = test[single_sensitive].values.tolist()\n",
    "\n",
    "    predicted, params, predictions_list =  \\\n",
    "        run_alg(algorithm, train, test, dataset, all_sensitive_attributes, single_sensitive,\n",
    "                privileged_vals, positive_val)\n",
    "\n",
    "    # make dictionary mapping sensitive names to sensitive attr test data lists\n",
    "    dict_sensitive_lists = {}\n",
    "    for sens in all_sensitive_attributes:\n",
    "        dict_sensitive_lists[sens] = test[sens].values.tolist()\n",
    "\n",
    "    sensitive_dict = processed_data.get_sensitive_values(tag)\n",
    "    one_run_results = []\n",
    "    for metric in get_metrics(dataset, sensitive_dict, tag):\n",
    "        result = metric.calc(actual, predicted, dict_sensitive_lists, single_sensitive,\n",
    "                             privileged_vals, positive_val)\n",
    "        one_run_results.append(result)\n",
    "\n",
    "    # handling the set of predictions returned by ParamGridSearch\n",
    "    results_lol = []\n",
    "    if len(predictions_list) > 0:\n",
    "        for param_name, param_val, predictions in predictions_list:\n",
    "            params_dict = { param_name : param_val }\n",
    "            results = []\n",
    "            for metric in get_metrics(dataset, sensitive_dict, tag):\n",
    "                result = metric.calc(actual, predictions, dict_sensitive_lists, single_sensitive,\n",
    "                                     privileged_vals, positive_val)\n",
    "                results.append(result)\n",
    "            results_lol.append( (params_dict, results) )\n",
    "\n",
    "    return params, one_run_results, results_lol\n",
    "\n",
    "def run_alg(algorithm, train, test, dataset, all_sensitive_attributes, single_sensitive,\n",
    "            privileged_vals, positive_val):\n",
    "    class_attr = dataset.get_class_attribute()\n",
    "    params = algorithm.get_default_params()\n",
    "\n",
    "    # Note: the training and test set here still include the sensitive attributes because\n",
    "    # some fairness aware algorithms may need those in the dataset.  They should be removed\n",
    "    # before any model training is done.\n",
    "    predictions, predictions_list =  \\\n",
    "        algorithm.run(train, test, class_attr, positive_val, all_sensitive_attributes,\n",
    "                      single_sensitive, privileged_vals, params)\n",
    "\n",
    "    return predictions, params, predictions_list\n",
    "\n",
    "\n",
    "def get_dict_sensitive_vals(dict_sensitive_lists):\n",
    "    \"\"\"\n",
    "    Takes a dictionary mapping sensitive attributes to lists in the test data and returns a\n",
    "    dictionary mapping sensitive attributes to lists containing each sensitive value only once.\n",
    "    \"\"\"\n",
    "    newdict = {}\n",
    "    for sens in dict_sensitive_lists:\n",
    "         sensitive = dict_sensitive_lists[sens]\n",
    "         newdict[sens] = list(set(sensitive))\n",
    "    return newdict\n",
    "\n",
    "def create_detailed_file(filename, dataset, sensitive_dict, tag):\n",
    "    return results.ResultsFile(filename, dataset, sensitive_dict, tag)\n",
    "    # f = open(filename, 'w')\n",
    "    # f.write(get_detailed_metrics_header(dataset, sensitive_dict, tag) + '\\n')\n",
    "    # return f\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Liste des algorithmes disponibles\n",
    "Il ya ceux classiques d'apprentissage sans correction de biais (base line) et ceux opérant par loyauté par une transformation des données, ou en contraignant l'étape d'apprentissage. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available algorithms:\n",
      "  SVM\n",
      "  GaussianNB\n",
      "  LR\n",
      "  DecisionTree\n",
      "  Kamishima\n",
      "  Calders\n",
      "  ZafarBaseline\n",
      "  ZafarFairness\n",
      "  ZafarAccuracy\n",
      "  Kamishima-accuracy\n",
      "  Kamishima-DIavgall\n",
      "  Feldman-SVM\n",
      "  Feldman-GaussianNB\n",
      "  Feldman-LR\n",
      "  Feldman-DecisionTree\n",
      "  Feldman-SVM-DIavgall\n",
      "  Feldman-SVM-accuracy\n",
      "  Feldman-GaussianNB-DIavgall\n",
      "  Feldman-GaussianNB-accuracy\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Calders'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_algorithm_names()[5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exécution\n",
    "`num_trials` exécutions pour un jeu de données et un algorithme déterminés. Les résultats sont empilés dans un ensemble de fichiers. \n",
    "\n",
    "**Attention** Certains algorithmes (Calders) plantent alors que d'autres (Feldman-SVM-DIavgall) sont excessivement longs. La cellule ci-dessous  a été exécutée pour différents algorihtmes; cf. ci-dessous."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datasets: 'adult'\n",
      "\n",
      "Evaluating dataset:adult\n",
      "Sensitive attribute:race\n",
      "    Algorithm: DecisionTree\n",
      "       supported types: {'numerical-binsensitive', 'numerical'}\n",
      "Results written to:\n",
      "    /home-local/pbesse/.fairness/results/adult_race_numerical-binsensitive.csv\n",
      "    /home-local/pbesse/.fairness/results/adult_race_numerical.csv\n",
      "Sensitive attribute:sex\n",
      "    Algorithm: DecisionTree\n",
      "       supported types: {'numerical-binsensitive', 'numerical'}\n",
      "Results written to:\n",
      "    /home-local/pbesse/.fairness/results/adult_sex_numerical-binsensitive.csv\n",
      "    /home-local/pbesse/.fairness/results/adult_sex_numerical.csv\n",
      "Sensitive attribute:race-sex\n",
      "    Algorithm: DecisionTree\n",
      "       supported types: {'numerical-binsensitive', 'numerical'}\n",
      "Results written to:\n",
      "    /home-local/pbesse/.fairness/results/adult_race-sex_numerical-binsensitive.csv\n",
      "    /home-local/pbesse/.fairness/results/adult_race-sex_numerical.csv\n"
     ]
    }
   ],
   "source": [
    "run(num_trials = 2, dataset = 'adult',algorithm = 'DecisionTree')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Résultats\n",
    "Tous les résultats intermédiaires sont stockés dans des fichiers archivés dans un répertoire masqué. Ces fichiers sont destinés à être lus par le programme `results.py` capable d'en extraire les valeurs utiles au calcul d'une liste très exhaustive de métriques. Ces derniers résultats sont également stockés puit l'exécution du programme `analysis.py` fournit les graphiques de l'article à l'aide de fonctions R (`ggplot`). \n",
    "\n",
    "Il s'agit donc, dans ce calepin, d'extraire de ces fichiers à titre d'exemple les indicateurs plus pertinents sans souci d'exhaustivité. *Soucis*: les intitulés ne sont pas très explicites et le nombre de modalités de la variable d'origine ethnique fait exploser la combinatoire de résultats possibles. Il semble important de se limiter à la seule prise en compte de variables sensibles binaires: genre ou origine ethnique caucasien *vs.* non caucasien. La prise en compte des interactions entre les deux variables sensibles introduit également une forte complexité pas indispensables en première lecture.\n",
    "\n",
    "Voici trois colonnes extraites des fichiers: `adult_sex_numerical-binsensitive.csv` et `adult_race_numerical-binsensitive.csv` qui en comporte 2 * 127 lorsque les variables sensibles sont considérées binaires. Dans le cas contraire, si toutes les modalités d'origine sont prises en compte, le fichier `adult_sex_numerical.csv` comporte 389 colonnes!\n",
    "\n",
    "\n",
    "\n",
    "|Algorithme | Paramètre | DI - genre| DI - origine|\n",
    "|----------|--------|-------|----|\n",
    "|SVM| none | 0.25|0.62|\n",
    "|SVM| none | 0.28|0.64|\n",
    "|GaussianNB| none | 0.32|0.62|\n",
    "|GaussianNB| none | 0.34|0.60|\n",
    "|LR | none | 0.33 |0.61|\n",
    "|LR | none | 0.30|0.61|\n",
    "|DecisionTree|none|0.44|0.70|\n",
    "|DecisionTree|none|0.43|0.60|\n",
    "|Kamishima|eta=1.0| 0.28|0.53|\n",
    "|Kamishima|eta=1.0|0.30|0.58|\n",
    "|ZafarFairness|\tc=0.001\t| 0.29|0.50|\n",
    "|ZafarFairness|\tc=0.001\t| 0.29|0.53|\n",
    "|Kamishima-DIavgall|eta=1.0| 0.28|0.53|\n",
    "|Kamishima-DIavgall|eta=1.0|0.30|0.58|\n",
    "|Feldman-SVM|lambda=1.0|0.30|0.68|\n",
    "|Feldman-SVM|lambda=1.0|0.37|0.62|\n",
    "|Feldman-SVM-DIavgall|lambda=0.35|0.30|0.81|\n",
    "|Feldman-SVM-DIavgall|lambda=0.45|0.35|0.68|\n",
    "\n",
    "**Remarques**\n",
    "- Compte tenu que seulement 2 exécutions ont été réalisées, les résultats semblent cohérents avec les graphiques de l'article (figure 6 page 16).\n",
    "- En revanche, les *DI* obtenus pour les algorithmes d'apprentissage seuls ne sont pas cohérents avec ceux calculés en R. A contrôler: est-ce dû aux différences entre les procédures de prétraitement?\n",
    "- Les corrections de biais vis-à-vis du genre ne sont pas très probantes pour les couples algo x paramètre exécutés par rapports aux algorithmes seuls qui constituent des *base lines*. C'est explicable vis-à-vis de la variable genre car l'optimisation des paramètres a sans doute été réalisée avec pour objectif de réduire le biais liée à la variable origine. Néanmoins même dans ce cas, cette réduction n'est pas exceptionnelle.\n",
    "- Il faudrait aussi ajouter la précision et choisir un indicateur de taux d'erreur conditionnel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparaison\n",
    "Comparer avec les résultats produits de façon plus simple voire naïve dans R:\n",
    "- calculer sur le fichier issu du prétraitement le *DI* d'origine: `genre` *vs.* `income`,\n",
    "- calculer la prévision par régression logistique,\n",
    "- calculer le *DI*,\n",
    "- faire la correciton élémentaire du seuil de décision."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Références\n",
    "Calders T., Verwer S. (2010). Three naive Bayes approaches for discrimination-free classification, Data Mining and Knowledge Discovery, 21 (2), pp 277–292.\n",
    "\n",
    "Friedler S., Scheidegger C., Venkatasubramanian S., Choudhary S., Hamilton E., Roth D. (2019). [A comparative study of fairness-enhancing interventions in machine learning](https://dl.acm.org/citation.cfm?id=3287589), Proceedings of the Conference on Fairness, Accountability, and Transparency."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
